{
	"playing atari with deep reinforcement learning": ["1607.05077", "1609.05521", "1605.05365", "1312.5602", "1507.06527", "1704.05539", "1605.01335"],
	"atari deep learning": ["1607.05077", "1609.05521", "1605.05365", "1312.5602", "1507.06527", "1704.05539", "1605.01335", "1507.08750", "1705.10998"],
	"rapidly-exploring random trees for safe path planning in continuous unknown environments": [],
	"deep reinforcement learning survey": ["1404.7828", "1701.07274"],
	"recurrent deep rl": ["1507.06527", "1609.05521"],
	"deep learning hyperparameter optimization": ["1406.3896", "1603.06560"],
	"deep q-learning on atari montezuma's revenge": ["1607.08316", "1502.05700", "1603.06560"],
	"atari game performance": ["1607.05077", "1704.05539"],
	"safe motion planning in dynamic, unknown, continuous environments": [],
	"obstacle trajectory prediction with limited-range noisy sensors": ["1601.04037"],
	"motion planning with uncertainty": ["1601.04037", "1504.08053", "1604.07446", "1607.06886", "1607.04788", "1605.04151"],
	"robot safe navigation in partially-known environments": ["1607.04788"],
	"__comment: There don't seem to be many survey papers on arXiv for these topics. Maybe I'm just looking in the wrong places": [],
	"rrt survey": [],
	"prm survey": [],
	"sampling-based navigation survey": [],
	"sampling planner survey": [],
	"sampling planner review": [],
	"sampling-based planning summary": [],
	"planning in a continuous environment": ["1305.5024"],
	"planning in a continuous configuration space": ["1305.5024"],
	"planning with a continuous time dimension": [],
	"safe navigation": ["1604.07446", "1607.06886", "1505.00023", "1305.5024"],
	"safe path planning": ["1604.07446", "1505.00023", "1305.5024"],
	"safe motion planning": ["1604.07446", "1607.06886", "1505.00023", "1305.5024"],
	"non-holonomic motion planning": []
}
